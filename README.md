# Fake-Job-Detection
Problem Statement 
•	The dataset is publicly available.
•	It contains total 17880 records and 18 features.
•	This dataset contains So we have student educational info like title, department, location, salay_range, etc.


The goal of the project is to predict whether a job posting is “FRAUDULENT” or not.
The Target variable is “fraudulent”.
We have to clean the data and apply all the algorithms to our model and check which one gives maximum accuracy.

Perform the following using “fake_job_postings.csv” 
Steps:
1)	Read the csv file and find df.shape, df.describe().
2)	Preprocess the data:-
1] Check the null values from the dataset
2] Fill the null values using median or mode.
3] Remove the insignificant columns from the dataset if necessary.
4] Convert the categorical columns to numerical columns.
5] Find the statistical information of the dataset.

3)	 Perform EDA on the dataset using visualization such as bar plot, Box  plot , count plot etc. .
4)	 Apply standard scaling to standardize the data if necessary.
5)	 Split the data into training and testing.
6)	 Apply 
Logistic regression,
Decision tree , 
Random forest, 
KNN & 
SVM 
to the same dataset and find :-
1] Accuracy of each model
2] Confusion matrix
3] Classification report
4] Deploy the model by giving same inputs to all the models and check the  accuracy and output of each model.

7)	Observe the  accuracy of each model and conclude which model gives the maximum accuracy. 
So finally the model which gives max accuracy will be the best model.
